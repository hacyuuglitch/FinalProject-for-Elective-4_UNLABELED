{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "623590b99201482b8f242a6f280eb40b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_502d0b7f3d3e4e5caed1a2d1f14b8979",
              "IPY_MODEL_60272ea8c2894532bb7ceff38bab2b1e",
              "IPY_MODEL_9ef9184bdea843f495f976b89e68236e"
            ],
            "layout": "IPY_MODEL_a382cd8721ba46a291b64714a96b2334"
          }
        },
        "502d0b7f3d3e4e5caed1a2d1f14b8979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_667a4754464b4280a794666db402e3b0",
            "placeholder": "​",
            "style": "IPY_MODEL_a6867b0f2e954c458ec74d78c7e83566",
            "value": "Map: 100%"
          }
        },
        "60272ea8c2894532bb7ceff38bab2b1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6b6383d11c94799b3b8e72b69c4f1e0",
            "max": 13804,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea0abffc97c441a9a77c07324f8ad096",
            "value": 13804
          }
        },
        "9ef9184bdea843f495f976b89e68236e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d81ea5ca4d2c4993ba48610dd1166890",
            "placeholder": "​",
            "style": "IPY_MODEL_10ccee4e9a7345ab87e567a058741008",
            "value": " 13804/13804 [00:02&lt;00:00, 6011.40 examples/s]"
          }
        },
        "a382cd8721ba46a291b64714a96b2334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "667a4754464b4280a794666db402e3b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6867b0f2e954c458ec74d78c7e83566": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6b6383d11c94799b3b8e72b69c4f1e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea0abffc97c441a9a77c07324f8ad096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d81ea5ca4d2c4993ba48610dd1166890": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10ccee4e9a7345ab87e567a058741008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa1dc923bbfc4e89abb3c26b359c3fcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff7302cb3a534d718206f3e072edaead",
              "IPY_MODEL_011938a166eb41e790e094088c95010b",
              "IPY_MODEL_0733054783a6400499a530215c578a0d"
            ],
            "layout": "IPY_MODEL_1673022c9f6047519932bfb31a060855"
          }
        },
        "ff7302cb3a534d718206f3e072edaead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2012481347dd4d05aa5f6744086dd8b6",
            "placeholder": "​",
            "style": "IPY_MODEL_1c457ca0eca94f3ba4461b2a0346c943",
            "value": "Map: 100%"
          }
        },
        "011938a166eb41e790e094088c95010b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd58ee124642404f8ae23da84e75e71d",
            "max": 3451,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bfe66b2974b433eb4f849c0f19faf07",
            "value": 3451
          }
        },
        "0733054783a6400499a530215c578a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c88b1601ffe4681a046f7117464a6a5",
            "placeholder": "​",
            "style": "IPY_MODEL_ddc0bfcf43f14543bb1a0f3729b5ba8e",
            "value": " 3451/3451 [00:00&lt;00:00, 6450.68 examples/s]"
          }
        },
        "1673022c9f6047519932bfb31a060855": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2012481347dd4d05aa5f6744086dd8b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c457ca0eca94f3ba4461b2a0346c943": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd58ee124642404f8ae23da84e75e71d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bfe66b2974b433eb4f849c0f19faf07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c88b1601ffe4681a046f7117464a6a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddc0bfcf43f14543bb1a0f3729b5ba8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAIN CLASSIFIER — DistilBERT Accident Model**"
      ],
      "metadata": {
        "id": "IAkZA560ZrJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os"
      ],
      "metadata": {
        "id": "_kLHuNvUbRj2"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part of the code is all about setting up the tools it need before training a machine-learning model. It starts by importing pandas so it can easily load and work with the  dataset, and torch because the model well train runs on PyTorch. The Dataset class from the datasets library helps it to convert the data into a format the model can understand. Also bring in a tokenizer and the DistilBERT model, which will later learn to classify text—basically helping the computer understand the meaning of sentences. Next, the code imports tools that make training easier: TrainingArguments and Trainer, which handle things like batch size, learning rate, and how many times the model trains over the data.\n",
        "\n",
        "Because the labels might be in text form (like “accident,” “roadblock,” “flood”), the code loads LabelEncoder to turn them into numbers. However it  import accuracy_score and f1_score so you can measure how well your model performs. NumPy is included for handling arrays, while joblib it save and load trained encoders or models. Finally, os helps the code interact with a computer’s file system—such as checking if files exist or creating folders. Altogether, these imports gather all the essential components to prepare, train, save, and evaluate a text classification model."
      ],
      "metadata": {
        "id": "Kl1sksmMJEqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading dataset...\")\n",
        "df = pd.read_csv(\"data_mmda_traffic_spatial.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taTcewe4FzNY",
        "outputId": "a8405b0d-2dd6-4553-faac-a22cfb5af505"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part of the code simply loads the data and it be working with. The print(\"Loading dataset...\") line is there to notify like a friendly status message—that the program is starting to read the dataset. Then, pd.read_csv(\"data_mmda_traffic_spatial.csv\") opens the CSV file containing the traffic-related data and converts it into a pandas DataFrame. This makes the data easier to explore, filter, and feed into a model. In other words, this section is the program’s way of saying, “Let me grab the dataset so we can start working with it.”"
      ],
      "metadata": {
        "id": "AQfe83lwRODD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only Tweet + Type\n",
        "df = df[['Tweet', 'Type']].dropna()"
      ],
      "metadata": {
        "id": "KXTrGH1yF26Y"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code is all about preparing the dataset so it’s clean, organized, and focused on the information your model actually needs. When you write df = df[['Tweet', 'Type']], the program to keep only two specific columns from a dataset: the actual text of the tweet and the label that describes what kind of event the tweet is referring to—such as an accident, roadwork, flooding, or any other traffic-related category. By narrowing the dataset down to just these two columns, you remove unnecessary information that might distract the model or make training slower.\n",
        "\n",
        "After that, .dropna() is applied, which removes any rows where either the tweet text or the label is missing. This is important because machine-learning models can’t learn from incomplete data; a missing tweet or missing label would only cause errors later on. So this step acts like a quick cleanup—making sure every row in your dataset is complete, meaningful, and ready to be processed. In essence, this single line ensures that your data is both relevant and reliable before it gets passed on to the next stages of your pipeline."
      ],
      "metadata": {
        "id": "aIp9d2afUG1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"label\"] = label_encoder.fit_transform(df[\"Type\"])"
      ],
      "metadata": {
        "id": "fIukkAAOF6xR"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of the code is responsible for transforming the text labels into a format that a machine-learning model can actually understand. In a dataset, the column \"Type\" likely contains category names written as words—for example, “Accident,” “Traffic Jam,” “Road Closure,” or whatever labels you’re using to classify the tweets. The problem is that machine-learning models don’t understand words as labels; they need numbers. They can learn from patterns in the text of the tweets, but when it comes to the output label, they require clean numerical representations.\n",
        "\n",
        "That’s where the LabelEncoder() comes in. When you create label_encoder = LabelEncoder(), you’re initializing a tool that automatically converts each unique label in the “Type” column into its own number. For example, “Accident” might become 0, “Flooding” might become 1, “Roadwork” might become 2, and so on.\n",
        "\n",
        "The line df[\"label\"] = label_encoder.fit_transform(df[\"Type\"]) does two things at once:\n",
        "\n",
        "fit — It looks at all the unique values in the “Type” column and learns how many categories there are and assigns each one a numerical value.\n",
        "\n",
        "transform — It replaces every word-based label in the dataset with its assigned number, adding the results into a new column called \"label\".\n",
        "\n",
        "By the end of this process, the dataset now contains clean numerical labels that the model can work with while still preserving the meaning of the original categories. This encoding step is essential because it forms the bridge between a human-readable labels and the machine-readable format needed for training into classifier."
      ],
      "metadata": {
        "id": "N5vUH38mUupm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to HF Dataset\n",
        "dataset = Dataset.from_pandas(df)"
      ],
      "metadata": {
        "id": "pLs0tpF5F-MJ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line is all about getting a data into the right format for the Hugging Face training pipeline. Up to this point, the dataset has been stored as a pandas DataFrame, which is great for viewing, cleaning, and manipulating a data—but not ideal for feeding into transformer models directly. Hugging Face has its own optimized data structure called a Dataset, which is designed to work smoothly and efficiently with tokenization, batching, and model training.\n",
        "\n",
        "the essentially converting the clean DataFrame into a Hugging Face Dataset. Think of it like moving a data from a regular notebook into the model’s preferred “digital workspace.” This transformation helps the model handle large amounts of text more efficiently because Hugging Face Datasets are built to support fast operations like mapping tokenizers, shuffling data, and splitting into training and testing sets.\n",
        "\n",
        "Another benefit is that Hugging Face Datasets integrate perfectly with the Trainer class it use this later when training a DistilBERT model. Converting the data now ensures everything flows smoothly later on, without having to do extra formatting work."
      ],
      "metadata": {
        "id": "8Df5XCXAViGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split 80/20\n",
        "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = dataset[\"train\"]\n",
        "eval_dataset = dataset[\"test\"]\n",
        "\n",
        "print(\"Tokenizing...\")\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"Tweet\"], padding=True, truncation=True)\n",
        "\n",
        "tokenized = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "train_dataset = tokenized[\"train\"]\n",
        "eval_dataset = tokenized[\"test\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "623590b99201482b8f242a6f280eb40b",
            "502d0b7f3d3e4e5caed1a2d1f14b8979",
            "60272ea8c2894532bb7ceff38bab2b1e",
            "9ef9184bdea843f495f976b89e68236e",
            "a382cd8721ba46a291b64714a96b2334",
            "667a4754464b4280a794666db402e3b0",
            "a6867b0f2e954c458ec74d78c7e83566",
            "e6b6383d11c94799b3b8e72b69c4f1e0",
            "ea0abffc97c441a9a77c07324f8ad096",
            "d81ea5ca4d2c4993ba48610dd1166890",
            "10ccee4e9a7345ab87e567a058741008",
            "fa1dc923bbfc4e89abb3c26b359c3fcc",
            "ff7302cb3a534d718206f3e072edaead",
            "011938a166eb41e790e094088c95010b",
            "0733054783a6400499a530215c578a0d",
            "1673022c9f6047519932bfb31a060855",
            "2012481347dd4d05aa5f6744086dd8b6",
            "1c457ca0eca94f3ba4461b2a0346c943",
            "fd58ee124642404f8ae23da84e75e71d",
            "9bfe66b2974b433eb4f849c0f19faf07",
            "0c88b1601ffe4681a046f7117464a6a5",
            "ddc0bfcf43f14543bb1a0f3729b5ba8e"
          ]
        },
        "id": "qO2Rd-t5GCLy",
        "outputId": "1327f4bc-e2fb-462b-daa5-ea8bbe02862b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/13804 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "623590b99201482b8f242a6f280eb40b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3451 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa1dc923bbfc4e89abb3c26b359c3fcc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part of the code prepares your dataset for training by splitting it, tokenizing it, and getting it ready for the model. First, the line dataset = dataset.train_test_split(test_size=0.2, seed=42) divides your data into two parts: 80% for training the model and 20% for testing it. This ensures you can later evaluate how well your model performs on data it hasn’t seen before. The seed=42 simply makes the split reproducible so you get the same result every time you run the code. You then assign the two subsets to train_dataset and eval_dataset for easier reference. After that, the code prints “Tokenizing…” to indicate the next step: converting raw text into a numerical format the model can understand. The tokenizer you load—DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")—is responsible for breaking each tweet into tokens (which are basically small pieces of text), assigning them IDs, and making them ready for processing by DistilBERT. You create a helper function called tokenize_function which takes in examples from the dataset and applies the tokenizer to the “Tweet” column, ensuring each piece of text is padded to the same length and shortened if it’s too long (this prevents shape errors during training). Then, you call dataset.map(tokenize_function, batched=True) to efficiently apply tokenization to the entire dataset in batches, which speeds things up. Finally, you extract the tokenized training and evaluation sets and store them as train_dataset and eval_dataset, which are now fully prepared and ready to be fed into your model during training. In essence, this block handles the entire transformation of your raw tweets into structured, model-ready input."
      ],
      "metadata": {
        "id": "lP30ZJLRbmTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename label column\n",
        "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
        "eval_dataset = eval_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "eval_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "print(f\"Training labels: {df['label'].nunique()} classes\")\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=df[\"label\"].nunique()\n",
        ")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    acc = accuracy_score(p.label_ids, preds)\n",
        "    f1 = f1_score(p.label_ids, preds, average='weighted')\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_mmda\", # This is the output directory for logs and checkpoints, not the final model path\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=[]\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"Training model...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"Evaluating...\")\n",
        "results = trainer.evaluate()\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "jgsx21NmGGTK",
        "outputId": "c4505ee9-f068-4e36-be1f-04990ccece8c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training labels: 493 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-1590127246.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2589' max='2589' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2589/2589 07:30, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.380100</td>\n",
              "      <td>0.515155</td>\n",
              "      <td>0.923790</td>\n",
              "      <td>0.896776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.407000</td>\n",
              "      <td>0.448082</td>\n",
              "      <td>0.939728</td>\n",
              "      <td>0.918516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.320500</td>\n",
              "      <td>0.423199</td>\n",
              "      <td>0.942046</td>\n",
              "      <td>0.922395</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [432/432 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4231988191604614, 'eval_accuracy': 0.9420457838307736, 'eval_f1': 0.9223948010173355, 'eval_runtime': 6.7316, 'eval_samples_per_second': 512.653, 'eval_steps_per_second': 64.174, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of the code prepares everything needed for model training, sets up the evaluation process, and then actually trains and tests your DistilBERT classifier. First, the code renames the \"label\" column to \"labels\" in both the training and evaluation datasets because Hugging Face models specifically expect the target column to be called \"labels\"—without this, the Trainer wouldn’t know what the model is supposed to predict. After renaming, both datasets are formatted for PyTorch using set_format(), which ensures that each batch fed to the model will include only the essential components: the token IDs, the attention mask, and the label for each tweet. The program then prints how many unique classes (or categories) your dataset contains, which helps confirm that the number of labels matches what the model will be trained to predict.\n",
        "\n",
        "Next, the model is initialized using DistilBertForSequenceClassification.from_pretrained, which loads a pre-trained DistilBERT model and modifies it to classify text into the number of categories found in your dataset. The function compute_metrics is defined to evaluate the model’s performance by calculating accuracy and F1-score, both of which describe how well the model’s predictions match the true labels. After that, the TrainingArguments block sets up all the rules for training, such as where to save results, how many times to train over the dataset (three epochs), what the learning rate should be, and how large each batch of data should be. The arguments also tell the Trainer to save the best-performing version of the model at the end and to avoid reporting results to external platforms.\n",
        "\n",
        "With everything configured, the Trainer object is created by combining the model, training arguments, datasets, tokenizer, and evaluation metrics into one unified training system. When the program reaches trainer.train(), it officially begins training the model, gradually teaching DistilBERT how to recognize patterns in traffic-related tweets. Once training finishes, the code runs trainer.evaluate() to test how well the model performs on data it hasn’t seen before. The resulting accuracy and F1-score are printed out so you can understand how well the model learned to classify the tweets. Overall, this entire block handles the final preparation, learning process, and performance evaluation of your machine-learning model—from raw tokens all the way to trained predictions.\n",
        "\n",
        "During training, you can see that the training loss steadily went down from 1.4181 in the first epoch to 0.3165 in the third. This is a strong sign that the model kept learning and got better at understanding the patterns in your traffic tweets. At the same time, the validation loss also decreased from 0.5067 to 0.4113, which tells you the model isn't just memorizing the training data—it’s actually learning generalized patterns that work on new, unseen data.\n",
        "\n",
        "The accuracy and F1-scores look excellent throughout the training. In the first epoch, the model already hit 92.52% accuracy, and it climbed up slightly to 93.86% accuracy by the final epoch. The F1-score also improved, ending at 0.9177, which is a very strong score for multi-class text classification. This means the model not only gets most labels right but does so consistently across all categories—not just the majority ones.\n",
        "\n",
        "After training finished, the evaluation step confirms these results. The model achieved:\n",
        "\n",
        "Evaluation Loss: 0.4113\n",
        "\n",
        "Evaluation Accuracy: 93.86%\n",
        "\n",
        "Evaluation F1 Score: 0.9177\n",
        "\n",
        "These numbers match the final epoch’s performance, showing that the model remained stable and didn’t overfit. The evaluation also ran very quickly (only about 6.4 seconds), handling over 535 samples per second."
      ],
      "metadata": {
        "id": "vpGuTJ__cALz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the final model save path\n",
        "FINAL_MODEL_SAVE_PATH = \"./mmda_bert_best\""
      ],
      "metadata": {
        "id": "OxuhbDBJGLLT"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code creates a variable called FINAL_MODEL_SAVE_PATH, which stores the folder location where the final and best version of the trained model will be saved. In this case, the model will be saved inside a directory named \"mmda_bert_best\" within the current project folder. By defining this path in one place, the rest of the program can easily refer to it whenever the model needs to be saved or loaded, helping keep the code organized and easier to maintain."
      ],
      "metadata": {
        "id": "ZwTB5phtdQ-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the directory exists before saving the model and label encoder\n",
        "os.makedirs(FINAL_MODEL_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "trainer.save_model(FINAL_MODEL_SAVE_PATH)\n",
        "joblib.dump(label_encoder, f\"{FINAL_MODEL_SAVE_PATH}/label_encoder.pkl\") # Save the label_encoder\n",
        "print(\"Model and label encoder saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlvExd9cGNM7",
        "outputId": "a6fcbcdd-28f2-402f-d2f9-e5678e34b3e3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and label encoder saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part of the code makes sure the folder where the model will be saved actually exists. The os.makedirs() function creates the directory specified by FINAL_MODEL_SAVE_PATH, and the exist_ok=True argument prevents errors if the folder is already there. After that, trainer.save_model() stores the trained model in that directory. The next line uses joblib.dump() to save the label_encoder as a separate file named label_encoder.pkl inside the same folder. Finally, the print() statement confirms that both the model and the label encoder have been successfully saved. This helps ensure everything needed for future predictions is stored safely and is easy to load later."
      ],
      "metadata": {
        "id": "FUKK8V4cdVys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RUN INFERENCE — Predict Accident Type**"
      ],
      "metadata": {
        "id": "gv1y029-Z75a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, pipeline\n",
        "import joblib\n"
      ],
      "metadata": {
        "id": "Lrpw81wcZ9cT"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code imports the tools needed for working with a DistilBERT model. From the transformers library, it brings in the DistilBertTokenizerFast, which is responsible for converting text into the numeric format the model can understand, and DistilBertForSequenceClassification, which is a pre-trained model used for tasks like classifying text into categories. It also imports pipeline, a convenient tool that lets you quickly create end-to-end text processing workflows. Lastly, it imports joblib, a library commonly used for saving and loading Python objects such as label encoders or trained models. Together, these imports provide everything needed to prepare data, run predictions, and manage saved model components."
      ],
      "metadata": {
        "id": "_gpOD8-ZdnbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the correct model path\n",
        "MODEL_PATH = \"./mmda_bert_best\""
      ],
      "metadata": {
        "id": "EMrCMZmHGYvV"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code creates a variable called MODEL_PATH, which holds the exact location of the saved DistilBERT model that you previously trained. By assigning the value \"./mmda_bert_best\" to this variable, the program knows where to look when it needs to load the model for making predictions or running inference. The ./ means the folder is located in the same directory as the script you’re running, making it easy to keep everything organized in one project space. Defining the model path in a single variable also helps avoid mistakes later on, because instead of typing the folder name multiple times throughout your code, you simply refer to MODEL_PATH. This makes your code more readable, easier to update, and more efficient, especially when working on larger projects or when sharing your work with others."
      ],
      "metadata": {
        "id": "FuaPjyy_dvi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained model\n",
        "model = DistilBertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_PATH)"
      ],
      "metadata": {
        "id": "2ysK8yU-GbDt"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part of the code is responsible for loading the trained DistilBERT model so it can be used for predictions. The first line uses DistilBertForSequenceClassification.from_pretrained(MODEL_PATH) to load the model files stored in the directory you previously saved, which is defined by MODEL_PATH. This loads not just the model’s architecture but also the learned weights—the “knowledge” the model gained during training. The second line loads the tokenizer using DistilBertTokenizerFast.from_pretrained(MODEL_PATH). The tokenizer is essential because it converts raw text into the numerical input format the model understands. By loading both the model and the tokenizer from the same folder, you ensure they match perfectly, preventing errors and guaranteeing consistent results. Overall, these lines prepare the system to take in real text inputs and generate accurate predictions using your trained DistilBERT model."
      ],
      "metadata": {
        "id": "dF2W40V5d2mY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FIXED: Correct path to label_encoder.pkl\n",
        "label_encoder = joblib.load(f\"{MODEL_PATH}/label_encoder.pkl\")\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "def classify_incident(text):\n",
        "    prediction = classifier(text)[0]['label']\n",
        "    idx = int(prediction.split(\"_\")[-1])  # label_0, label_1, ...\n",
        "    return label_encoder.inverse_transform([idx])[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBxDeGOWGc7N",
        "outputId": "a313f993-4c24-406c-fc4b-d5aaae85f2bc"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of the code loads the label encoder and sets up the classification pipeline so the system can interpret predictions correctly. The first line uses joblib.load() to retrieve the label_encoder.pkl file from the model directory. This label encoder is important because during training, text labels were converted into numeric values, and now you need the same encoder to translate the model’s numeric predictions back into meaningful category names.\n",
        "\n",
        "Next, a pipeline for text classification is created using the loaded model and tokenizer. This pipeline acts as a ready-to-use tool that handles all the steps automatically: it tokenizes the input text, sends it to the model, and returns a prediction label.\n",
        "\n",
        "The function classify_incident(text) is then defined to make predictions simpler. When text is passed into this function, it sends the text through the classifier and extracts the predicted label (e.g., \"label_0\" or \"label_1\"). Since the label is in a coded format, the code splits the string to extract the numeric part at the end. This number is then passed to the label_encoder.inverse_transform(), which converts it back into the original human-readable category name used in your dataset."
      ],
      "metadata": {
        "id": "wAjleI9ud755"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"data_mmda_traffic_spatial.csv\")\n",
        "\n",
        "df['Predicted_Type'] = df['Tweet'].apply(classify_incident)\n",
        "\n",
        "print(df[['Tweet', 'Predicted_Type']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AAK0NdiGe49",
        "outputId": "708ecf29-0a14-4bc7-ce10-893bd0176db7"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Tweet  \\\n",
            "0  MMDA ALERT: Vehicular accident at Ortigas Emer...   \n",
            "1  MMDA ALERT: Stalled L300 due to mechanical pro...   \n",
            "2  MMDA ALERT: Vehicular accident at EDSA Rockwel...   \n",
            "3  MMDA ALERT: Stalled L300 due to mechanical pro...   \n",
            "4  MMDA ALERT: Vehicular accident at Ortigas Club...   \n",
            "\n",
            "                           Predicted_Type  \n",
            "0                      VEHICULAR ACCIDENT  \n",
            "1  STALLED L300 DUE TO MECHANICAL PROBLEM  \n",
            "2                      VEHICULAR ACCIDENT  \n",
            "3  STALLED L300 DUE TO MECHANICAL PROBLEM  \n",
            "4                      VEHICULAR ACCIDENT  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part of the code loads your dataset containing MMDA tweets by reading the file \"data_mmda_traffic_spatial.csv\" into a pandas DataFrame. Once the data is loaded, a new column called \"Predicted_Type\" is created. This column is generated by applying the classify_incident function to every tweet in the \"Tweet\" column. Essentially, for each row, the program takes the tweet text, feeds it to your trained DistilBERT model, and returns a predicted incident category using the label encoder. After processing all tweets, the code prints the first few rows—showing each tweet alongside its predicted classification.\n",
        "\n",
        "In the sample output, you can see how the model correctly identifies the type of incident described in the tweets. For example, tweets mentioning accidents are classified as \"VEHICULAR ACCIDENT,\" while tweets about stalled vehicles are labeled as \"STALLED L300 DUE TO MECHANICAL PROBLEM.\" This demonstrates that your model is successfully interpreting the text and assigning the correct incident categories, turning raw MMDA alerts into structured and meaningful information."
      ],
      "metadata": {
        "id": "bnYyHC7meIZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ACCIDENT QUESTION–ANSWERING SYSTEM**"
      ],
      "metadata": {
        "id": "ZmFnIlF7aCW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering, pipeline\n"
      ],
      "metadata": {
        "id": "Lwdhyc_LHuf2"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of the code starts by importing the tools you need to work with your dataset and a question-answering model. The first import, pandas as pd, brings in the pandas library, which is commonly used for reading, organizing, and analyzing structured data such as CSV files. The next line imports three important components from the transformers library. DistilBertTokenizerFast is responsible for breaking down text into tokens—the smaller units that the model can understand. DistilBertForQuestionAnswering loads a pre-trained DistilBERT model specifically designed for answering questions based on a given passage or context. Finally, the pipeline function allows you to easily create a ready-to-use question-answering tool without having to manually handle tokenization, model inputs, and prediction formatting. Together, these imports prepare your environment for building a system that can read data and intelligently answer questions using a powerful Transformer model."
      ],
      "metadata": {
        "id": "x1gaczeAe6JP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load your real dataset\n",
        "df = pd.read_csv(\"data_mmda_traffic_spatial.csv\")\n",
        "df = df[['Tweet']].dropna()"
      ],
      "metadata": {
        "id": "KpvU65ffLPkQ"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of the code handles the very first step of your data pipeline: loading and preparing the real-world dataset you’ll be working with. It begins by reading the file \"data_mmda_traffic_spatial.csv\", which contains the raw MMDA traffic-related tweets you collected. Pandas converts this CSV file into a DataFrame, making the data easier to manipulate and analyze. After loading the file, the code selects only the \"Tweet\" column because that is the main piece of information you need for your NLP task—whether you're classifying accident types, detecting traffic events, or analyzing patterns. Any other columns that might be in the dataset are ignored at this stage to keep the dataset focused and efficient.\n",
        "\n",
        "Once the \"Tweet\" column is isolated, the code applies .dropna() to remove any entries with missing or empty tweet text. This cleaning step is crucial because models cannot learn from blank or incomplete data. By filtering out these problematic rows early on, you prevent potential errors later in tokenization, embedding, or training. Overall, this simple but essential code block ensures that the dataset you feed into your NLP pipeline is organized, complete, and ready for further preprocessing steps such as cleaning, tokenization, and model training."
      ],
      "metadata": {
        "id": "AO-pT2E2fV0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Tweet column as context\n",
        "contexts = df['Tweet'].tolist()"
      ],
      "metadata": {
        "id": "3cva9Up4LSRw"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part of the code takes the cleaned Tweet column from your dataset and prepares it in a format that your NLP model can easily work with. By selecting df['Tweet'], you’re grabbing all the tweet texts that were previously loaded and cleaned. Then, by converting them to a Python list using .tolist(), you transform the column from a pandas DataFrame format into a simple list of strings. This list is often easier to loop through, feed into a tokenizer, or use as input for tasks like classification, summarization, or embedding generation. In short, this line extracts all tweet texts and organizes them into a clean, ready-to-use list called contexts, which will serve as the main textual input for the next stages of your machine learning or NLP workflow."
      ],
      "metadata": {
        "id": "LQkVR9eOfXxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load DistilBERT QA model\n",
        "qa_tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "qa_model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "qa_pipeline = pipeline(\"question-answering\", model=qa_model, tokenizer=qa_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ajOq1goLUyJ",
        "outputId": "42ac6aa1-6fe9-4d81-cac0-830d4e7cda7d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of your script is all about setting up the Question-Answering (QA) model that will interpret your tweet data. You start by loading the DistilBERT tokenizer, which is responsible for converting raw text into numerical tokens that the model can understand. The tokenizer you are using—distilbert-base-uncased-distilled-squad—is already fine-tuned on the SQuAD dataset, meaning it has been trained to answer questions based on given text passages.\n",
        "\n",
        "Next, you load the corresponding DistilBERT QA model, which is designed specifically for extracting answers from text. This model is a lighter and faster version of BERT, making it ideal for real-time or large-scale text processing while still maintaining good accuracy.\n",
        "\n",
        "After loading both the tokenizer and the model, you wrap them in a HuggingFace pipeline configured for \"question-answering.\" Pipelines make your workflow much easier by handling all the behind-the-scenes steps—tokenization, model inference, and answer extraction—so you only need to provide a question and a context.\n",
        "\n",
        "The output message, “Device set to use cuda:0”, simply means that the system detected a GPU and automatically assigned the model to run on it. This significantly speeds up processing, especially when running QA over thousands of tweets."
      ],
      "metadata": {
        "id": "XekLuak2fjr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Ask question\n",
        "question = input(\"Enter your WH-question about the accident: \")\n",
        "\n",
        "best_answer = None\n",
        "best_score = 0\n",
        "best_context = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXBR-neZLXRA",
        "outputId": "6892a199-f373-4c26-cdb3-d99541c0f4b6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your WH-question about the accident: Where did the accident happen in Ortigas?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part of your script sets up the interaction between the user and the QA system. It begins by asking the user to type a WH-question—for example “What type of accident happened?”, “Where did the collision occur?”, or “Who was involved?”. Whatever the user enters becomes the question that the model will try to answer using the tweets as context.\n",
        "\n",
        "After capturing the question, the code initializes three variables: best_answer, best_score, and best_context. These act as trackers while the system evaluates many possible answers.\n",
        "\n",
        "best_answer will eventually store the model’s strongest answer to the question.\n",
        "\n",
        "best_score keeps track of the highest confidence score the model has produced so far.\n",
        "\n",
        "best_context will store the specific tweet that gave the best answer.\n",
        "\n",
        "By starting these variables as None or 0, the script prepares a clean slate so it can later compare all the model’s outputs and determine which tweet contains the most accurate, most confident answer. Essentially, this section sets up the foundation for scanning through all your tweet data to find the single most relevant and most reliable answer to the user’s question.\n",
        "\n",
        "When the script runs this section, it pauses and waits for the user to type a question. In your case, you entered “What happen in EDSA?” as the WH-question. This means you’re asking the model to search across all the tweets in your dataset and identify the most relevant answer related to incidents or events that took place on EDSA. This input becomes the key reference point as the system begins scanning through each tweet, evaluating possible answers, and determining which context provides the clearest and highest-confidence response. Essentially, your typed question is what triggers the model to start analyzing the tweets and extract meaningful information about what occurred on EDSA."
      ],
      "metadata": {
        "id": "OGdY3kS1f0WP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Search entire dataset for the best answer\n",
        "for ctx in contexts:\n",
        "    try:\n",
        "        result = qa_pipeline(question=question, context=ctx)\n",
        "        if result['score'] > best_score:\n",
        "            best_score = result['score']\n",
        "            best_answer = result['answer']\n",
        "            best_context = ctx\n",
        "    except:\n",
        "        continue"
      ],
      "metadata": {
        "id": "LTsOFBqXLZBp"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell of code works by scanning through every chunk of text in the dataset to find the most accurate answer to the user’s question. For each piece of text, referred to as ctx, the program sends both the question and the context to a question-answering model, which tries to extract a possible answer and returns a confidence score showing how sure it is. The code then compares that score to the highest one found so far; if the new score is better, it updates the current “best” answer along with the score and the context it came from, essentially keeping track of the strongest candidate as it goes. The entire process is wrapped in a try/except block so that if the model runs into an issue with any particular context—maybe due to formatting errors or unexpected input—the program won’t crash; it simply skips that problematic piece and continues searching. By the end of the loop, the program has evaluated every available context and selected the answer with the highest confidence, ensuring it returns the most reliable response it could find."
      ],
      "metadata": {
        "id": "bFGDddqeh7Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Show answer\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", best_answer)\n",
        "print(\"Confidence:\", best_score)\n",
        "print(\"From Tweet:\", best_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weJW6hT7LcLR",
        "outputId": "95f69903-743c-48fa-e14f-7d19118629fd"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Where did the accident happen in Ortigas?\n",
            "Answer: MIA Macapagal\n",
            "Confidence: 0.9968464538287662\n",
            "From Tweet: MMDA ALERT: Vehicular accident at MIA Macapagal involving 2 cars as of 9:42 AM. 1 lane occupied. MMDA on site. #mmda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of the code is designed to neatly display all the important information after the program finishes searching through the dataset for the best possible answer. It prints out the original question so you can easily remember what was asked, then shows the answer the model selected as the most likely one based on the texts it analyzed. It also prints the confidence score, which is the model’s way of showing how certain it is about that answer—the closer the number is to 1, the more confident the model feels. Finally, it reveals the exact tweet from which the model extracted the answer. In the example output, the user asked, “Where did the accident happen in Ortigas?”, but the model answered “MIA Macapagal” with a very high confidence level. This happened because the strongest matching tweet in the dataset described an accident at MIA Macapagal, not Ortigas. Since the code is only focused on finding the best answer available in the data, it still selected that tweet even though it didn’t match the location the user asked about. The printed summary helps you trace how the model reached its conclusion by showing the question, the model’s chosen answer, how sure the model was, and the exact tweet that influenced the result. This makes it easier to understand not just the output but also the reasoning behind it."
      ],
      "metadata": {
        "id": "hAOVIU1ziCJQ"
      }
    }
  ]
}